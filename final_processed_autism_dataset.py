# -*- coding: utf-8 -*-
"""Final_Processed_Autism_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rXeUZ2QgsXJ_2XJyEDaCLPk_Z19Gv1KG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

import numpy as np
print(np.__version__)

import pandas as pd
print(pd.__version__)

!pip install --upgrade numpy scikit-learn

file_path = '/content/drive/My Drive/Final_Combined_Autism_Dataset.csv'

df = pd.read_csv(file_path)

df.head()

print(df.shape)

from google.colab import drive
drive.mount('/content/drive')

df.info()

print(df.isnull().sum())

!pip install missingno seaborn matplotlib

import seaborn as sns

import matplotlib.pyplot as plt

import missingno as msno

missing_before = df.isnull().sum()

if missing_before.sum() > 0:
    plt.figure(figsize=(10,5))
    missing_before[missing_before > 0].plot(kind='bar', color='red')
    plt.title("Missing Values Before Cleaning")
    plt.xlabel("Columns")
    plt.ylabel("Count of Missing Values")
    plt.xticks(rotation=45)
    plt.show()
else:
    print(" No missing values before cleaning.")

for col in df.columns[:19]:
    print(f"Column: {col}")
    print(df[col].value_counts())
    print(f" Count of '?' values: {df[col].isin(['?']).sum()}")
    print("-" * 50)

import numpy as np

df.replace('?', np.nan, inplace=True)

df['Age_Months'] = pd.to_numeric(df['Age_Months'], errors='coerce')
df['Qchat_score'] = pd.to_numeric(df['Qchat_score'], errors='coerce')

df.loc[:, 'Age_Months'] = df['Age_Months'].fillna(df['Age_Months'].median())
df.loc[:, 'Qchat_score'] = df['Qchat_score'].fillna(df['Qchat_score'].median())

print(df.columns.tolist())

for col in df.columns:
    print(f"Column: {col}")
    print(df[col].unique())
    print("-" * 40)

# Convert to lowercase and strip spaces for consistency
df['Ethnicity'] = df['Ethnicity'].str.lower().str.strip().str.replace("'", "").str.replace('"', '')

# Mapping similar values to a unified form
ethnicity_map = {
    'white european': 'white european',
    'white-european': 'white european',
    'black': 'black',
    'black ': 'black',
    'asian': 'asian',
    'south asian': 'south asian',
    'south asian\'': 'south asian',
    'south asian ': 'south asian',
    'middle eastern': 'middle eastern',
    'middle eastern\'': 'middle eastern',
    'middle eastern ': 'middle eastern',
    'native indian': 'native indian',
    'others': 'others',
    'other': 'others',
    'latino': 'latino',
    'mixed': 'mixed',
    'pacifica': 'pacifica',
    'pasifika': 'pacifica',
    'turkish': 'turkish',
}

# Apply the mapping
df['Ethnicity'] = df['Ethnicity'].replace(ethnicity_map)

for col in df.columns:
    print(f"Column: {col}")
    print(df[col].unique())
    print("-" * 40)

# Convert to lowercase and strip spaces
df['Class/ASD Traits'] = df['Class/ASD Traits'].str.lower().str.strip()

# Optionally, map to binary values for modeling
df['Class/ASD Traits'] = df['Class/ASD Traits'].map({'no': 0, 'yes': 1})

# Step 1: Standardize text (lowercase, strip whitespace/quotes)
df['Who _completed _the _test'] = df['Who _completed _the _test'].str.lower().str.strip().str.replace("'", "").str.replace('"', '')

# Step 2: Map variations to unified categories
who_map = {
    'family member': 'family member',
    'parent': 'parent',
    'relative': 'relative',
    'self': 'self',
    'others': 'others',
    'health care professional': 'health care professional',
}

# Step 3: Apply mapping
df['Who _completed _the _test'] = df['Who _completed _the _test'].replace(who_map)

print(df['Who _completed _the _test'].unique())

# Loop through each column and fill NaN with the mode
for col in df.columns:
    mode_value = df[col].mode()[0]
    df[col] = df[col].fillna(mode_value)

df.isnull()

df.head()

for col in df.columns:
    print(f"Column: {col}")
    print(df[col].unique())
    print("-" * 40)

categorical_cols = ['Jaundice', 'Family_mem_with_ASD', 'Sex']
for col in categorical_cols:
    df.loc[:, col] = df[col].fillna(df[col].mode()[0])

print("\nMissing Values Count (After Cleaning):\n", df.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming df is your cleaned DataFrame
# 1. Distribution of ASD Traits
plt.figure(figsize=(6,4))
sns.countplot(x='Class/ASD Traits', data=df, palette='coolwarm')
plt.title("Distribution of ASD vs Non-ASD Cases")
plt.xlabel("ASD Traits (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

# 2. Age Distribution
plt.figure(figsize=(8,5))
sns.histplot(df['Age_Months'], bins=20, kde=True, color='skyblue')
plt.title("Age Distribution of Participants")
plt.xlabel("Age (in Months)")
plt.ylabel("Frequency")
plt.show()

# 3. Gender vs ASD
plt.figure(figsize=(6,4))
sns.countplot(x='Sex', hue='Class/ASD Traits', data=df, palette='Set2')
plt.title("ASD Traits by Gender")
plt.xlabel("Gender (0 = Female, 1 = Male)")
plt.ylabel("Count")
plt.legend(title="ASD Traits")
plt.show()

# 4. Correlation Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix of Features")
plt.show()

# 5. Boxplot of Qchat Scores by ASD Trait
plt.figure(figsize=(8,5))
sns.boxplot(x='Class/ASD Traits', y='Qchat_score', data=df, palette='pastel')
plt.title("Qchat Score Distribution by ASD Class")
plt.xlabel("ASD Traits")
plt.ylabel("Qchat Score")
plt.show()

# 6. Ethnicity Distribution
plt.figure(figsize=(10,5))
df['Ethnicity_white european'] = df.get('Ethnicity_white european', 0)  # if one-hot encoded
sns.barplot(x=df['Ethnicity_white european'].value_counts().index,
            y=df['Ethnicity_white european'].value_counts().values)
plt.title("Sample Count of White European Ethnicity (example)")
plt.xlabel("Encoded Ethnicity")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

# Heatmap for missing values (After Cleaning)
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)
plt.title("Missing Values Heatmap (After Cleaning)")
plt.show()

# Missingno matrix plot (After Cleaning)
msno.matrix(df)
plt.title("Missing Values Matrix (After Cleaning)")
plt.show()

# Missingno bar plot (After Cleaning)
msno.bar(df)
plt.title("Missing Values Count (After Cleaning)")
plt.show()

df.drop(columns=['Case_No'], inplace=True)

# First, ensure column names are clean (remove leading/trailing spaces)
df.columns = df.columns.str.strip()

# Apply One-Hot Encoding to both columns and drop the first category to avoid multicollinearity
df = pd.get_dummies(df, columns=['Ethnicity', 'Who _completed _the _test'], drop_first=True)

from sklearn.preprocessing import LabelEncoder

label_cols = ['Sex', 'Jaundice', 'Family_mem_with_ASD']
le = LabelEncoder()

for col in label_cols:
    df[col] = le.fit_transform(df[col])

import pandas as pd

# Create a cross-tab of counts, using 'Sex' instead of 'gender'
table_counts = pd.crosstab(df['Sex'], df['Class/ASD Traits'])

# Add a 'Total' column
table_counts['Total'] = table_counts.sum(axis=1)

# Normalize to get row-wise percentages
table_percentages = pd.crosstab(df['Sex'], df['Class/ASD Traits'], normalize='index') * 100

# Add 'Total' column (100% for each row)
table_percentages['Total'] = 100

# Display both
print("üî¢ Count Table:")
print(table_counts)
print("\nüìä Percentage Table (Row-wise):")
print(table_percentages.round(1))

plt.figure(figsize=(6,4))
sns.countplot(x=df['Class/ASD Traits'])
plt.title("Autism Classification Distribution")
plt.xlabel("ASD Traits (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), cmap='coolwarm', annot=True, fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()

plt.figure(figsize=(8,5))
sns.histplot(df['Age_Months'], bins=20, kde=True)
plt.title("Age Distribution")
plt.xlabel("Age in Months")
plt.ylabel("Count")
plt.show()

#from sklearn.preprocessing import MinMaxScaler

#scaler = MinMaxScaler()
#df[['Age_Months', 'Qchat_score']] = scaler.fit_transform(df[['Age_Months', 'Qchat_score']])

print(df.columns)



!pip install imbalanced-learn

from collections import Counter

from imblearn.over_sampling import SMOTE

class_counts = df['Class/ASD Traits'].value_counts()

# Plot class distribution before SMOTE
plt.figure(figsize=(6,4))
sns.barplot(x=class_counts.index, y=class_counts.values, palette="viridis")
plt.xlabel("Class Labels")
plt.ylabel("Count")
plt.title("Class Distribution Before SMOTE")
plt.show()

print("Class Distribution Before SMOTE:\n", class_counts)

df['Class/ASD Traits'].fillna(df['Class/ASD Traits'].mode()[0], inplace=True)

X = df.drop(columns=['Class/ASD Traits'])
y = df['Class/ASD Traits']

# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class/ASD Traits'])], axis=1)

print("SMOTE applied successfully after filling NaN values!")

class_counts_after = df_resampled['Class/ASD Traits'].value_counts() # Add parentheses to call the function
plt.figure(figsize=(6,4))
sns.barplot(x=class_counts_after.index, y=class_counts_after.values, palette="coolwarm")
plt.xlabel("Class Labels")
plt.ylabel("Count")
plt.title("Class Distribution After SMOTE")
plt.show()

print("Class Distribution After SMOTE:\n", class_counts_after)

print(df.shape)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# ‚úÖ Ensure only clean features are used (adjust column list if needed)
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

# ‚úÖ 70-30 Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,          # ‚Üê changed from 0.2 to 0.3
    random_state=42,
    stratify=y
)

# ‚úÖ Logistic Regression
log_reg = LogisticRegression(max_iter=500, random_state=42)
log_reg.fit(X_train, y_train)

# ‚úÖ Predictions
y_pred = log_reg.predict(X_test)

# ‚úÖ Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# ‚úÖ Output
print(f"Model Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print("Confusion Matrix:")
print(conf_matrix)

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np

# ‚úÖ Define the Logistic Regression confusion matrix
cm_list = [[198, 14],
           [10, 202]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Create the plot using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD'])
# Set cmap to 'Blues'
disp.plot(cmap='Blues', values_format='d', ax=plt.gca())

# ‚úÖ Styling
plt.title("üìà Logistic Regression Confusion Matrix") # Updated title for Logistic Regression
plt.grid(False)
plt.tight_layout()
plt.show()

df.head()

# Convert all boolean values to integers (if not already)
df = df.astype(int)

df.head()

train_accuracy = log_reg.score(X_train, y_train)
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
print(f"Test Accuracy: {accuracy * 100:.2f}%")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# ‚úÖ 1. Separate features and target
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

# ‚úÖ 2. Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# ‚úÖ 3. (Optional) Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ‚úÖ 4. Define tuned Random Forest model
clf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=4,
    max_features='sqrt',
    random_state=42
)

# ‚úÖ 5. Fit on training data
clf.fit(X_train_scaled, y_train)

# ‚úÖ 6. Predict & evaluate on test data
y_pred = clf.predict(X_test_scaled)
print("‚úÖ Test Accuracy:", accuracy_score(y_test, y_pred))
print("‚úÖ Classification Report:\n", classification_report(y_test, y_pred))
print("‚úÖ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ‚úÖ 7. Perform 5-fold cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(clf, scaler.fit_transform(X), y, cv=cv)

print("‚úÖ Cross-validation scores:", cv_scores)
print("‚úÖ Mean accuracy:", np.mean(cv_scores))
print("‚úÖ Std deviation:", np.std(cv_scores))

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np

# ‚úÖ Define the Random Forest confusion matrix
cm_list = [[317, 1],
           [1, 316]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Create the plot using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD'])
# Set cmap to 'Blues'
disp.plot(cmap='Blues', values_format='d', ax=plt.gca())

# ‚úÖ Styling
plt.title("üå≤ Random Forest Confusion Matrix") # Updated title for Random Forest
plt.grid(False)
plt.tight_layout()
plt.show()

print(X_train.columns.tolist())  # The DataFrame you used to train the model

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Step 1: Split the dataset with stratification
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Step 2: Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Define weak hyperparameter grid to reduce accuracy
param_grid = {
    'C': [0.1],                  # Default regularization strength
    'kernel': ['rbf'],         # RBF kernel (default)
    'gamma': ['scale'],        # Default gamma ('scale')
}

# Step 4: Initialize and run GridSearchCV
grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)
grid.fit(X_train_scaled, y_train)

# Step 5: Best model and prediction
best_svm = grid.best_estimator_
y_train_pred = best_svm.predict(X_train_scaled)
y_test_pred = best_svm.predict(X_test_scaled)

# Step 6: Evaluation
print("\n‚úÖ Best Hyperparameters:", grid.best_params_)
print(f"Training Accuracy: {accuracy_score(y_train, y_train_pred) * 100:.2f}%")
print(f"Testing Accuracy:  {accuracy_score(y_test, y_test_pred) * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

# Step 7: Cross-validation on full dataset
X_scaled = scaler.fit_transform(X)
cv_scores = cross_val_score(best_svm, X_scaled, y, cv=5)
print("\nüìä Cross-validation scores:", cv_scores)
print(f"Mean CV accuracy: {cv_scores.mean():.4f}")
print(f"Standard deviation: {cv_scores.std():.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np

# ‚úÖ Define the SVC confusion matrix
cm_list = [[300, 18],
           [31, 286]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Create the plot using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD'])
# Keep cmap as 'Blues'
disp.plot(cmap='Blues', values_format='d', ax=plt.gca())

# ‚úÖ Styling
plt.title("üíú SVC Confusion Matrix") # Updated title for SVC
plt.grid(False)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
import numpy as np

# Step 1: Split the dataset with stratification
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Step 2: Scale features (optional for XGBoost, but included for consistency)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Define hyperparameter grid
param_grid = {
    'n_estimators': [10],
    'max_depth': [1],
    'learning_rate': [0.001],
    'subsample': [0.5],
    'colsample_bytree': [0.5],
    'reg_alpha': [5],
    'reg_lambda': [50]
}

# Step 4: Initialize and run GridSearchCV
xgb_clf = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

grid = GridSearchCV(xgb_clf, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)
grid.fit(X_train_scaled, y_train)

# Step 5: Best model and prediction
best_xgb = grid.best_estimator_
y_train_pred = best_xgb.predict(X_train_scaled)
y_test_pred = best_xgb.predict(X_test_scaled)

# Step 6: Evaluation
print("\n‚úÖ Best Hyperparameters:", grid.best_params_)
print(f"Training Accuracy: {accuracy_score(y_train, y_train_pred) * 100:.2f}%")
print(f"Testing Accuracy:  {accuracy_score(y_test, y_test_pred) * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

# Step 7: Cross-validation on full dataset
X_scaled = scaler.fit_transform(X)
cv_scores = cross_val_score(best_xgb, X_scaled, y, cv=5)
print("\nüìä Cross-validation scores:", cv_scores)
print(f"Mean CV accuracy: {cv_scores.mean():.4f}")
print(f"Standard deviation: {cv_scores.std():.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np

# ‚úÖ Define the XGBoost confusion matrix
cm_list = [[285, 33],
           [59, 258]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Create the plot using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD'])
# Set cmap to 'Blues'
disp.plot(cmap='Blues', values_format='d', ax=plt.gca())

# ‚úÖ Styling
plt.title("üå≥ XGBoost Confusion Matrix")
plt.grid(False)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Step 1: Split the dataset with stratification
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Step 2: Scale features (StandardScaler is generally not required for Naive Bayes, but added for consistency)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Initialize Naive Bayes classifier
nb = GaussianNB()

# Step 4: Train the model
nb.fit(X_train_scaled, y_train)

# Step 5: Predictions
y_train_pred = nb.predict(X_train_scaled)
y_test_pred = nb.predict(X_test_scaled)

# Step 6: Accuracy Calculation
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

# Step 7: Evaluation
print("\nTraining Accuracy: {:.2f}%".format(train_accuracy * 100))
print("Testing Accuracy:  {:.2f}%".format(test_accuracy * 100))
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

# Step 8: Cross-validation on full dataset
X_scaled = scaler.fit_transform(X)
cv_scores = cross_val_score(nb, X_scaled, y, cv=5)
print("\nüìä Cross-validation scores:", cv_scores)
print(f"Mean CV accuracy: {cv_scores.mean():.4f}")
print(f"Standard deviation: {cv_scores.std():.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np

# Define the confusion matrix as a list
cm_list = [[315, 3],
      [159, 158]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# Plot the confusion matrix using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD'])
# Change cmap to 'Blues' or any other colormap string
disp.plot(cmap='Blues', values_format='d', ax=plt.gca())

# Style and show
plt.title("üìâ Naive Bayes Confusion Matrix")
plt.grid(False)
plt.tight_layout()
plt.show()

print(X_train.shape, X_test.shape)

import matplotlib.pyplot as plt

# Calculate sizes and percentages
train_size = X_train.shape[0]
test_size = X_test.shape[0]
total = train_size + test_size
train_pct = (train_size / total) * 100
test_pct = (test_size / total) * 100

# Plotting
plt.figure(figsize=(8, 5))
bars = plt.bar(['Training Set', 'Test Set'], [train_size, test_size], color=['#4e79a7', '#f28e2b'])

# Add value and percentage labels
for bar, size, pct in zip(bars, [train_size, test_size], [train_pct, test_pct]):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 10,
             f'{size} ({pct:.1f}%)', ha='center', fontsize=11, fontweight='bold')

# Styling
plt.title('üìä Train-Test Split Distribution', fontsize=14, weight='bold')
plt.ylabel('Number of Samples', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.xticks(fontsize=11)
plt.tight_layout()
plt.show()

print(" Target column name:", y.name)
print("\n Feature columns in X:")
print(X.columns)

if y.name in X.columns:
    print("WARNING: Target label is included in features (X). This will cause data leakage.")
else:
    print(" No label leakage detected. Target is not in features.")

df_corr = df.corr()
print(df_corr['Class/ASD Traits'].sort_values(ascending=False))

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np

# Step 1: Split the data (stratified to preserve class balance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

#  Step 2: Scale features (fit on training, transform on both)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  Step 3: Define the ANN model with regularization and early stopping
ann_model = MLPClassifier(
    hidden_layer_sizes=(50, 30),      # Simpler architecture
    max_iter=500,
    alpha=0.001,                      # L2 regularization to reduce overfitting
    random_state=42,
    early_stopping=True,             # Stops if validation score doesn't improve
    validation_fraction=0.1,         # 10% of training data used for validation
    n_iter_no_change=10              # Patience
)

# Step 4: Train the model
ann_model.fit(X_train_scaled, y_train)

#  Step 5: Predict and evaluate on the test set
ann_preds = ann_model.predict(X_test_scaled)

print("ANN Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, ann_preds) * 100:.2f}%")
print("Confusion Matrix:\n", confusion_matrix(y_test, ann_preds))
print("Classification Report:\n", classification_report(y_test, ann_preds))

#  Step 6: Cross-validation to check generalization
# Note: Always scale the full dataset before cross-validation!
X_scaled = scaler.fit_transform(X)
cv_scores = cross_val_score(ann_model, X_scaled, y, cv=5)

print("Cross-Validation Accuracy Scores:", cv_scores)
print("Mean Accuracy:", np.mean(cv_scores))
print("Std Deviation:", np.std(cv_scores))

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np # Import numpy is already present but good to emphasize

# ‚úÖ Define the ANN confusion matrix
cm_list = [[295, 23],
      [20, 297]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Plot the confusion matrix
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD']) # Use the numpy array here
disp.plot(cmap='Purples', values_format='d', ax=plt.gca())

# ‚úÖ Style the plot
plt.title("üß† ANN Confusion Matrix")
plt.grid(False)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
import numpy as np

# ‚úÖ Split & scale
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ‚úÖ Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "SVM": SVC(kernel='linear', probability=True, random_state=42),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "ANN": MLPClassifier(hidden_layer_sizes=(50, 30), max_iter=500, early_stopping=True, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# ‚úÖ Plot ROC curves
plt.figure(figsize=(12, 8))
colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'magenta', 'brown']

for (name, model), color in zip(models.items(), colors):
    model.fit(X_train_scaled, y_train)
    y_proba = model.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})", color=color, linewidth=2)

# ‚úÖ Add baseline
plt.plot([0, 1], [0, 1], 'k--', label='No Skill (AUC = 0.50)', alpha=0.7)

# ‚úÖ Style the plot
plt.title("üìä ROC Curve Comparison of Machine Learning Models", fontsize=16, fontweight='bold')
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
plt.legend(loc="lower right", fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier

# ‚úÖ Split & scale the data
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ‚úÖ Define models (excluding Naive Bayes, which we‚Äôll add manually)
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "SVM": SVC(kernel='linear', probability=True, random_state=42),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "ANN": MLPClassifier(hidden_layer_sizes=(50, 30), max_iter=500, early_stopping=True, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# ‚úÖ Store metrics
model_names = []
accuracies = []
precisions = []
recalls = []
f1_scores = []

# ‚úÖ Train & evaluate all models
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    model_names.append(name)
    accuracies.append(accuracy_score(y_test, y_pred))
    precisions.append(precision_score(y_test, y_pred))
    recalls.append(recall_score(y_test, y_pred))
    f1_scores.append(f1_score(y_test, y_pred))

# ‚úÖ Add Naive Bayes results manually
model_names.append("Naive Bayes")
accuracies.append(0.7986)     # Testing accuracy
precisions.append(0.86)       # Weighted precision
recalls.append(0.80)          # Weighted recall
f1_scores.append(0.79)        # Weighted F1-score

# ‚úÖ Plot grouped bar chart
x = np.arange(len(model_names))  # Label positions
width = 0.2

colors = {
    'Accuracy': '#1f77b4',   # Blue
    'Precision': '#ff7f0e',  # Orange
    'Recall': '#2ca02c',     # Green
    'F1-Score': '#d62728'    # Red
}

plt.figure(figsize=(14, 7))
plt.bar(x - 1.5 * width, accuracies, width, label='Accuracy', color=colors['Accuracy'])
plt.bar(x - 0.5 * width, precisions, width, label='Precision', color=colors['Precision'])
plt.bar(x + 0.5 * width, recalls, width, label='Recall', color=colors['Recall'])
plt.bar(x + 1.5 * width, f1_scores, width, label='F1-Score', color=colors['F1-Score'])

plt.ylabel('Scores')
plt.title('üìä Performance Metrics of Machine Learning Models', fontsize=14, fontweight='bold')
plt.xticks(x, model_names, rotation=45, ha='right')
plt.ylim(0.75, 1.05)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

print("Total samples:", len(df_resampled))

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Initialize the Decision Tree with tuned hyperparameters
dt_model = DecisionTreeClassifier(
    criterion='gini',           # or 'entropy' for information gain
    max_depth=5,                # limit depth to avoid overfitting
    min_samples_split=10,      # minimum samples required to split
    min_samples_leaf=5,        # minimum samples per leaf node
    random_state=42
)

# 2. Train the model
dt_model.fit(X_train, y_train)

# 3. Predict on the test data
dt_preds = dt_model.predict(X_test)

# 4. Evaluate the model
print("Decision Tree Results (with hyperparameter tuning):")
print(f"Accuracy: {accuracy_score(y_test, dt_preds) * 100:.2f}%")
print("Classification Report:\n", classification_report(y_test, dt_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, dt_preds))

from sklearn.metrics import confusion_matrix, accuracy_score

# Predict using your decision tree model
y_pred = dt_model.predict(X_test)

# Get confusion matrix and accuracy
cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

# Print formatted output
print("Confusion Matrix:")
print(f"           Predicted No   Predicted Yes")
print(f"Actual No     {cm[0][0]}              {cm[0][1]}")
print(f"Actual Yes    {cm[1][0]}              {cm[1][1]}")
print(f"\nAccuracy: {accuracy:.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np # Import numpy is already present but good to emphasize

# ‚úÖ Use the exact confusion matrix
cm_list = [[317, 1],
      [0, 317]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Create the display using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD']) # Use the numpy array here
disp.plot(cmap='Greens', values_format='d', ax=plt.gca())

# ‚úÖ Styling
plt.title("üå≥ Decision Tree Confusion Matrix")
plt.grid(False)
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', edgecolor='k')
plt.title("PCA of Features")
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split # Import train_test_split


# Assuming 'df_resampled' is your DataFrame
X = df_resampled.drop(columns=['Class/ASD Traits'])
y = df_resampled['Class/ASD Traits']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# 1. Initialize the KNN model
knn_model = KNeighborsClassifier(
    n_neighbors=5,
    metric='minkowski',
    p=2
)

# 2. Train the model
knn_model.fit(X_train, y_train)

# 3. Make predictions
knn_preds = knn_model.predict(X_test)

# 4. Evaluate the model
print("üéØ KNN Results:")
print(f"Accuracy: {accuracy_score(y_test, knn_preds) * 100:.2f}%")
print("Classification Report:\n", classification_report(y_test, knn_preds))
print("Confusion Matrix:\n", confusion_matrix(y_test, knn_preds))

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
import numpy as np # Import numpy

# ‚úÖ Confusion matrix values from model
cm_list = [[307, 4],
           [26, 298]]

# Convert the list to a NumPy array
cm_array = np.array(cm_list)

# ‚úÖ Create the plot using the NumPy array
plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_array, display_labels=['No ASD', 'ASD']) # Use the numpy array here
disp.plot(cmap='Blues', values_format='d', ax=plt.gca())

# ‚úÖ Styling
plt.title("üìä KNN Confusion Matrix")
plt.grid(False)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Data
data = {
    'Model': [
        'Decision Tree', 'Logistic Regression', 'Artificial Neural Net',
        'SVM', 'KNN', 'Random Forest', 'XGBoost', 'Naive Bayes'
    ],
    'Accuracy (%)': [99.84, 94.34, 93.23, 89.76, 95.28, 84.33, 85.51, 74.49],
    'Rating': [
        'Excellent', 'Excellent', 'Excellent',
        'Very Good', 'Very Good', 'Good',
        'Moderate', 'Low'
    ]
}

# Create DataFrame
df_accuracy = pd.DataFrame(data)

# Show styled table
def color_rating(val):
    colors = {
        'Excellent': 'background-color: lightgreen',
        'Very Good': 'background-color: lightblue',
        'Good': 'background-color: khaki',
        'Moderate': 'background-color: orange',
        'Low': 'background-color: lightcoral'
    }
    return colors.get(val, '')

styled_table = df_accuracy.style.applymap(color_rating, subset=['Rating'])
styled_table.set_properties(**{'text-align': 'center'})
styled_table

import matplotlib.pyplot as plt
import pandas as pd

# ‚úÖ Define data
data = {
    'Model': [
        'Decision Tree', 'Logistic Regression', 'Artificial Neural Net',
        'KNN', 'SVM', 'XGBoost', 'Random Forest', 'Naive Bayes'
    ],
    'Accuracy': [99.84, 94.34, 93.23, 95.28, 89.76, 85.51, 84.33, 74.49],
    'Rating': [
        'Excellent', 'Excellent', 'Excellent',
        'Very Good', 'Very Good', 'Moderate',
        'Good', 'Low'
    ]
}

# ‚úÖ Convert to DataFrame and sort
df = pd.DataFrame(data)
df = df.sort_values(by='Accuracy', ascending=True)

# ‚úÖ Color map for ratings
rating_colors = {
    'Excellent': '#4CAF50',     # Green
    'Very Good': '#2196F3',     # Blue
    'Good': '#FFC107',          # Amber
    'Moderate': '#FF9800',      # Orange
    'Low': '#F44336'            # Red
}

colors = df['Rating'].map(rating_colors)

# ‚úÖ Plot
plt.figure(figsize=(10, 6))
bars = plt.barh(df['Model'], df['Accuracy'], color=colors)

# ‚úÖ Annotate bars with accuracy values
for bar, acc in zip(bars, df['Accuracy']):
    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,
             f'{acc:.2f}%', va='center', fontsize=10)

# ‚úÖ Styling
plt.xlabel('Accuracy (%)')
plt.title('üìä Model Accuracy and Ratings')
plt.xlim(70, 102)
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()

# ‚úÖ Show plot
plt.show()

import pandas as pd

# Create a cross-tab of counts, using 'Sex' instead of 'gender'
table_counts = pd.crosstab(df['Sex'], df['Class/ASD Traits'])

# Add a 'Total' column
table_counts['Total'] = table_counts.sum(axis=1)

# Normalize to get row-wise percentages
table_percentages = pd.crosstab(df['Sex'], df['Class/ASD Traits'], normalize='index') * 100

# Add 'Total' column (100% for each row)
table_percentages['Total'] = 100

# Display both
print("üî¢ Count Table:")
print(table_counts)
print("\nüìä Percentage Table (Row-wise):")
print(table_percentages.round(1))

print("‚úÖ Model expects input shape:", best_svm.n_features_in_)

try:
    print("‚úÖ Model input feature names:")
    print(best_svm.feature_names_in_)
except AttributeError:
    print("‚ö†Ô∏è Your model does not store feature names (some models like SVC don‚Äôt).")

df.head()

import joblib
joblib.dump(scaler, 'scaler.pkl')

from google.colab import files
files.download('scaler.pkl')

import pickle

# ‚úÖ Save the 'model' variable (RandomForestClassifier)
with open('autism_classifier.pkl', 'wb') as f:
    pickle.dump(model, f)  # Changed 'your_model_variable' to 'model'

# Download the model
from google.colab import files
files.download('autism_classifier.pkl')

